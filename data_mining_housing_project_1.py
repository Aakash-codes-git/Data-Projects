# -*- coding: utf-8 -*-
"""Data mining Housing Project 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fApPIymGeOhcrtVo5e8reFmesckas40v
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

df = pd.read_csv('train.csv')

df.columns.values

print(df.corr())

# check for null value
df.isnull().sum()

# imputing null values
df_1 = df.copy()
df_1['LotFrontage'] = df['LotFrontage'].fillna(value=0)
df_1['GarageYrBlt'] = df['GarageYrBlt'].fillna(value=df_1['GarageYrBlt'].median())
df_1['MasVnrArea'] = df['MasVnrArea'].fillna(value=0)

# droping predictors ' OverallCondition' because contains subjective praisal opinion
df_2 = df_1.copy()
df_2 = df_1.drop(columns='OverallCond')

# dropping predictors with correlation  < 0.1 and > -0.1 to SalePrices
df_2.corr()['SalePrice']
df_3 = df_2.copy()
drop_list = ['Id', 'BsmtFinSF2', 'LowQualFinSF', 'BsmtHalfBath', '3SsnPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']
df_3 = df_2.drop(columns=drop_list)

df_3.corr()['SalePrice']

# Group categorical and numerical columns
df_4 = df_3.copy()
cvar_list = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'RoofStyle', 'RoofMatl', 'Exterior1st',
             'Exterior2nd', 'MasVnrType','ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional',
             'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']

df_4[cvar_list] = df_4[cvar_list].astype('category')

nvar_list = ['LotFrontage', 'LotArea', 'OverallQual', 'YearBuilt','YearRemodAdd', 'MasVnrArea',
             'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'BsmtFullBath',
             'FullBath', 'HalfBath', 'BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','Fireplaces', 'GarageYrBlt',
             'GarageCars', 'GarageArea', 'WoodDeckSF','EnclosedPorch','ScreenPorch', 'OpenPorchSF', 'SalePrice']
df_4[nvar_list] = df_4[nvar_list].astype('float64')

df_5 = df_4.copy()
# Dummy code
df_5 = pd.get_dummies(df_5, prefix_sep='_')

# standardization of numeric value
df_5[nvar_list] = (df_5[nvar_list] - df_5[nvar_list].mean())/df_5[nvar_list].std()
print(df_5.columns.values)
df_6 = df_5.copy()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso, LassoCV

# split training and test data
test_size = 0.2
df_partition = df_6
df_nontest, df_test = train_test_split(df_partition, test_size=test_size, random_state=2)

dv = 'SalePrice'

#separating depend value
y = df_nontest[dv]
X = df_nontest.drop(columns=[dv])

# optimal LASSO
k_fold = 10
clf_optimal = LassoCV(cv=k_fold, random_state=2, n_jobs=-1).fit(X,y)
print(clf_optimal.alpha_)

# defining function for model coefficient
def summary_coef(model_object):
  n_predictors = X.shape[1]
  model_coef = pd.DataFrame(model_object.coef_.reshape(1,n_predictors), columns = X.columns.values)
  model_coef['Intercept'] = model_object.intercept_
  return model_coef.transpose()


print(summary_coef(clf_optimal))

# using test partition to test the optimal model, (Average squared error)
y_test_actual = df_test[dv]
x_test = df_test.drop(columns = [dv])
y_test_predicted_optimal = clf_optimal.predict(x_test)
n_obs_test = df_test.shape[0]
ASE_test = sum((y_test_actual - y_test_predicted_optimal)**2)/n_obs_test
print(ASE_test)

# using training partition to test the model, (Average squared error)
y_nontest_actual = y
y_nontest_predicted = clf_optimal.predict(X)
n_obs_nontest = df_nontest.shape[0]
ASE_nontest = sum((y_nontest_actual - y_nontest_predicted)**2) / n_obs_nontest
print(ASE_test)
print(ASE_nontest)

from google.colab import files
uploaded = files.upload()

df_newdata = pd.read_csv('testtt.csv')

npredictor_list = nvar_list.copy()
npredictor_list.remove(dv)
print(npredictor_list)

# Setting new data datatype
df_newdata_sample1 = df_newdata.copy()
df_newdata_sample1[cvar_list] = df_newdata[cvar_list].astype('category')
df_newdata_sample1[npredictor_list] = df_newdata[npredictor_list].astype('float64')

# standardize numerical variables
historical_sample_mean = df_5[nvar_list].mean()
historical_sample_std = df_5[nvar_list].std()

df_newdata_sample2 = df_newdata_sample1.copy()
df_newdata_sample2[npredictor_list] = (df_newdata_sample1[npredictor_list] - historical_sample_mean[npredictor_list])/historical_sample_std[npredictor_list]

# get dummies for new data
df_newdata_sample3 = pd.get_dummies(df_newdata_sample2, prefix_sep= '_')

# matching column vlaues with our model
for i in range(0, len(X.columns.values)):
  if X.columns.values[i] not in df_newdata_sample3.columns.values:
    df_newdata_sample3[X.columns.values[i]] = 0
# remove the columns that was included in the mode value, match the column values of the data
for i in df_newdata_sample3.columns.values:
  if i not in X.columns.values:
    df_newdata_sample3 = df_newdata_sample3.drop(columns=i)

predicted_standardizedFare = clf_optimal.predict(df_newdata_sample3)

predicted_standardizedFare

x = 1215.26626/1385

x -1

